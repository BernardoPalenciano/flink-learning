# Proyecto de Procesamiento de Streams con Apache Flink

Este repositorio contiene un proyecto de ejemplo para practicar el desarrollo de un pipeline de streaming de datos en tiempo real. La arquitectura utiliza **Apache Kafka** como fuente (Source) y **PostgreSQL** como destino (Sink), con **Apache Flink** realizando la transformaci√≥n de datos.

## üöÄ Arquitectura del Proyecto

El entorno se orquesta completamente con Docker Compose y consta de cuatro servicios principales:

| Servicio | Componente | Hostname Interno | Puerto Externo |
| :--- | :--- | :--- | :--- |
| `broker` | Apache Kafka (Modo KRaft) | `kafka-broker` | `9092` |
| `jobmanager` | Apache Flink Job Manager | `flink-jobmanager` | `8081` |
| `taskmanager` | Apache Flink Task Manager | `flink-taskmanager` | N/A |
| `postgres` | PostgreSQL (Base de Datos Sink) | `flink-postgres` | `5432` |

---

## üõ†Ô∏è Requisitos Previos

Aseg√∫rate de tener instalado y configurado lo siguiente en tu sistema (Windows):

1. ¬†**Docker Desktop:** Para ejecutar Docker Compose.
2. ¬†**Java/JDK:** Para compilar el c√≥digo de Flink (generalmente Java 11 o superior).
3. ¬†**Python 3.x:** Necesario para ejecutar los scripts de utilidad (productores de datos).
¬† ¬† * **Dependencia Python:** Instala la librer√≠a `kafka-python`: `pip install kafka-python` (Necesaria solo si se usa el script productor de datos desde el host).

---

## üèÅ Pasos para Arrancar el Proyecto

Sigue estos pasos en orden para poner en marcha el entorno y el Job de Flink:


### Paso 1: Build. Compilaci√≥n sin instalar Maven
Para compilar el proyecto en tu m√°quina local sin instalar Maven, usamos un contenedor de Maven como ejecutor de compilaci√≥n.

**Requisito:** Tener Docker Desktop iniciado.

**Comando de Compilaci√≥n:**

Ejecuta el siguiente comando desde la ra√≠z del proyecto (donde se encuentra el `pom.xml`):

```bash
docker run --rm -v "$(pwd):/usr/src/app" -w /usr/src/app maven:3-eclipse-temurin-17 mvn clean package
```

Este comando descarga una imagen de Maven (3-eclipse-temurin-17), montatu c√≥digo local ($(pwd)), y ejecuta mvn clean package dentro del contenedor. El resultado es la creaci√≥n del archivo JAR ejecutable en tu carpeta local: target/flink-session-job-1.0-SNAPSHOT.jar

### Paso 2: Despliegue y Ejecuci√≥n del Job

Una vez compilado el JAR, el job se despliega en el Job Manager. Para ello primero debemos levantar los servicios:

```bash
# Navegar al directorio donde se encuentra el docker-compose.yml
cd infra

# Construir las im√°genes custom y levantar todos los contenedores en modo detached (-d)
# Esto crea una imagen de Kafka con Python y la librer√≠a 'kafka-python'.
docker compose up -d --build

```
A continuaci√≥n, subimos y ejecutamos el JAR
1. Abre la interfaz web de Flink en tu navegador: http://localhost:8081
2. Sube el JAR (target/flink-session-job-1.0-SNAPSHOT.jar) usando el bot√≥n "Submit New Job" o utiliza la CLI de Flink.


### Paso 3: Configuraci√≥n de Source y Sink (Creaci√≥n de Topic y Tabla)
Una vez que los contenedores est√©n operativos, usa la CLI de Docker para ejecutar los comandos dentro de los contenedores.

A. Crear el Topic de Kafka (user_activity)
El script create_topic.py se ejecuta dentro del contenedor kafka-broker. Esto garantiza que la conexi√≥n use el nombre interno (kafka-broker:9092) de manera fiable.

Comando de Ejecuci√≥n (desde el directorio infra/):

```bash
# El script est√° montado en la ruta /scripts dentro del contenedor
docker exec kafka-broker python3 /scripts/create-topic.py
```

Verificaci√≥n:
El script esperar√° 20 segundos y si es exitoso, mostrar√°: ‚úÖ Topic 'user_activity' creado con √©xito.

B. Creaci√≥n de la Tabla de PostgreSQL (aggregated_counts)
La tabla de destino de Flink se crea autom√°ticamente al inicio del servicio de la base de datos, mapeando el archivo create_table.sql en el docker-compose.yml. -> esto no he conseguido que funcione por lo que hay que crear la tabla manualmente

```bash
# Entramos en el docker de postgres-db
 docker exec -it postgres-db psql -U user -d flinkdb
# Ejecutamos el sql
CREATE TABLE IF NOT EXISTS session_results (
    user_id VARCHAR(255) NOT NULL,
    total_clicks BIGINT,
    session_start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    -- CLAVE PRIMARIA COMPUESTA: Es √∫nica por usuario Y por hora de inicio de sesi√≥n
    PRIMARY KEY (user_id, session_start_time) 
);

```
